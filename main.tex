\input{style.tex}

\begin{document}

%%
%% begin titlepage
%%

\begin{titlepage}
\newgeometry{left=6cm}
\definecolor{titlepage-color}{HTML}{2196F3}
\newpagecolor{titlepage-color}\afterpage{\restorepagecolor}
\newcommand{\colorRule}[3][black]{\textcolor[HTML]{#1}{\rule{#2}{#3}}}
\begin{flushleft}
\noindent
\\[-1em]
\color[HTML]{FFFFFF}
\makebox[0pt][l]{\colorRule[1976D2]{1.3\textwidth}{8pt}}
\par
\noindent

{ \setstretch{1.4}
\vfill
\noindent {\huge \textbf{\textsf{The Raft Consensus Algorithm}}}
\vskip 2em
\noindent
{\Large \textsf{\MakeUppercase{Fabio Anderegg}}
\vfill
}

\textsf{2017-12-04}}
\end{flushleft}
\end{titlepage}
\restoregeometry

%%
%% end titlepage
%%


{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
\pagebreak
}

\section{Abstract}

Raft is a consensus algorithm for distributed systems. It aims to be easier to understand and implemented in real-world software than Paxos, an older and widely used algorithm.

This paper explains the algorithm from the bottom up, first describing all the components and
then explaining how they work together to solve specific scenarios.

The paper describes the behaviour of the algorithm, especially the reaction to network messages
in great detail, including discussing edge cases. This should help programmers to implement
the algorithm because it requires much less cross-referencing information from multiple chapters in
comparison to the original paper and PhD thesis.

\section{Introduction}

This paper covers Raft, a consensus algorithm for distributed systems.

Raft wants to be a simpler replacement for Paxos, an algorithm developed by Leslie Lamport and considered the reference consensus algorithm for distributed systems\cite{paxos}. Paxos suffers from two big problems, which Raft tries to solve: It is hard to understand \cite[p.91]{raft_phd_thesis}, has some missing pieces for a real world system and is generally hard to implement\cite{paxos_made_live}.

Raft was developed by Diego Ongaro and his colleagues at Stanford university and extensively described in this PhD thesis \cite{raft_phd_thesis}. He not only describes how the algorithm works, but also did a study under students to test if the algorithm is actually easier to understand than Paxos. Raft has been formally verified \cite{raft_proof} and implemented in multiple real world systems, e.g. in etcd \cite{etcd_raft}, Consul \cite{consul_raft} and RethinkDB \cite{rethinkdb_raft}.

The official GitHub Page \cite{raft_github} contains many further references about the algorithm, a simulator for the algorithm, from which screen shots have been used in this paper, and an extensive list of implementations in many different programming languages.

This paper has three main parts: The first part explains what consensus in a distributed system means, the second part explains how the Raft algorithm works in detail, and the third part shows how the algorithm reacts to some specific scenarios.
The pictures for the scenarios sections were created using Raftscope\cite{raftscope}, a software to visualize the state of Raft-based distributed system.

This paper describes the latest version of Raft as described in Diego Ongaros PhD dissertation \cite{raft_phd_thesis}. Compared to the papers published before the dissertation, the dissertation describes the client-cluster interaction much more clearly and contains a simpler cluster membership change algorithm.

\subsection{What is consensus?}

Consensus in a distributed system means that multiple servers agree on the same specific state of the system at a specific time. The decision reached on the state by the system at this specific time can not be changed later on, but the system can agree on another state at a later time, superseding the old state.

Clients interacting with such a system see a single consistent state, independent of the server they are talking to.

This problem is hard to solve because most components participating in the system (servers, network, clocks, disk...) are inherently unreliable. They can not only fail and be complete unusable, they can also be slow, mangle data in transit (network packets ordering) or contain invalid data (bit flipping in RAM).

Many consensus algorithms (including Raft) are based on a majority system. If a majority of the servers agree on a specific value, that value is considered the "true" value of the system.

\subsection{The CAP Theorem}
The CAP theorem describe how certain failures in a distributed system affect the system. It states that a distributed system can never provide more than two of the following guarantees: \cite{cap_theorem}

\begin{itemize}
  \item \textbf{Consistency}: Operations in the system must appear as if they happened one after another in total order. At a specific time all servers must return the same, most recent write to the system.
  \item \textbf{Availability}: Every working node in the system must be able to return a response to a request.
  \item \textbf{Partition tolerance}: The system works even when the network drops packets, loses nodes or is partitioned.
\end{itemize}

Raft is a CP algorithm, that means systems using this algorithm are tolerant to partitions and always return a consistent view of the data in the system. Raft sacrifices the availability guarantee, which means that at least a majority of the servers must be working and be able to communicate with each other for the system to be usable.

\section{The algorithm}
A Raft cluster consists of nodes. A node is a server running the Raft software and communicating with the other nodes in the cluster over a network connection.
Every one has a unique ID which is used in the election process. In every cluster, one node is elected as a leader node, which coordinates the operations inside the cluster. A leader can only be elected by
a majority of the cluster nodes. This means the minimal cluster size is three nodes, and bigger clusters should always have an uneven count of nodes (In a cluster
of six nodes, two nodes can crash and a majority can still be found, this is not better than a five node cluster, where also two nodes can crash without impacting the cluster).
This also means that when more than half of the cluster nodes are offline, the cluster is inoperable.
This availability guarantee of the CAP theorem the Raft algorithm gives up for the other two guarantees.

Every leader is elected for a specific term. A term is a positive integer, incremented every time a node starts an election process by proposing itself as a new leader to the other nodes.

The current leader sends out a heartbeat network packet in some preset interval. If a follower node does not receive a heartbeat within a given interval (which should be a bit longer than the interval in which the leader sends out the heartbeats to account for the network round-trip time), the node considers the leader to be down and proposes itself as a new leader.

The main data structure of the algorithm as a replicated, append-only log. It contains the data the clients wants to save in the system and also some metadata
for the algorithm itself.

The Raft algorithm is designed to survive the following and many other failure cases:
\begin{itemize}
    \item The current leader crashes
    \item Network connections between nodes fails
    \item Network connections are very slow or drop messages between nodes
\end{itemize}

\subsection{State}
Every node in the Raft cluster is always in one of the following three states:
\begin{itemize}
    \item Leader: The node is the leader of the cluster
    \item Candidate: The node did not receive a heartbeat from a leader, it is waiting to get enough votes to be a leader itself
    \item Follower: Some other node is a leader
\end{itemize}


\begin{figure}
  \captionsetup{labelformat=empty}
  \caption{Transition between node states \cite{fuzzing_raft_for_fun_and_publication} }
  \centering
  \includegraphics[scale=0.25]{raft_fsm}
  
\end{figure}




\subsubsection*{Persistent data written to disk}

This data has to be saved on disk before answering to the network request coming from another node. The fsync or equivalent system call should be used on the file descriptor to make sure the data has arrived on the disk and not just the write caches of the operating system.

\begin{tabularx}{\textwidth}{ | p{80px} | X | }
\hline
\textbf{Property} & \textbf{Description} \\ \hline
currentTerm & The election term the node thinks is the current one \\ \hline
votedFor & For which node the current node has voted in this term \\ \hline
log[] & The replicated log containing all the data clients want to save in the system. The log is append only, only new entries can be append but old ones can not be changed.
Every log entry contains an index (starting at one) and the term in which the data was written. \\ \hline
\end{tabularx}

\subsubsection*{Volatile data}

This data is not written to disk and initialized on node startup.

\begin{tabularx}{\textwidth}{ | p{80px} | X | }
\hline
\textbf{Property} & \textbf{Description} \\ \hline
commitIndex & Up to which index in the replicated log the current node considers to be committed (commited means a majority of the cluster
received the data entry, saved it to disk and notified the leader). On startup, this information is initialized from the leaders heartbeat message (AppendEntries). \\
\hline
lastApplied & The highest index in the replicated log, initialized when reading the log entries from disk on startup. \\
\hline
receivedVotes[] & Vote responses per node id, a vote could be granted or rejected \\
\hline
\end{tabularx}

\subsubsection*{Leader data}

This data is initialized when the node gets elected as leader and is not persisted to disk.
It has to be cleared every time the node gets elected and must not be reused in later terms.

\begin{tabularx}{\textwidth}{ | p{80px} | X | }
\hline
\textbf{Property} & \textbf{Description} \\ \hline
nextIndex[] & Index of the next entry the leader will send to the specific follower. This map/associative array is initialized to the leaders last log entry index + 1 on leader election.
This map is used when a followers log falls behind the leaders log and the leader starts to send missing log entries to the follower. \\ \hline
matchIndex[] & Index of the latest acknowledged (written to disk) log entry for the specific follower. Initialized to zero on leader election. \\ \hline
\end{tabularx}

\subsection{Timers}
The Raft server needs two timers which when they run out trigger some action.

\subsubsection*{Election timer}
After a leader has been elected, it will send out heartbeat messages to all followers. This heartbeat
will restart the election timer running on the followers. If the timer runs out because the follower did not 
receive a heartbeat, the node will switch into candidate state and begin the election process to become a leader itself.
The election timer is inactive when the node is a leader.

The actual timeout in milliseconds has to be defined randomly. The minimum should be the network round-trip time between two nodes plus some margin,
and on top of that a random offset. The timeout should not be too high, because it influences the time the system requires to reelect a new leader
if the old one crashes. The minimal timeout and the random range should be configurable, because it highly depends on the environment where the algorithm runs
(network latency, server latency/performance etc.).

The actual timeout amount has to be random to avoid split votes where multiple nodes switch into candidate mode at the same time and the votes get split
up between these nodes. If this happens it is possible that no node receives the majority of the votes and the election process has to be restarted,
making the time where the cluster has no leader longer. This can still happen with randomized timeouts, but the chances are much smaller.

\subsubsection*{Candidate and leader timer}
This timer replaces the election timer when the node is a leader. In candidate state, both timers are active.
The timeout amount has to be smaller than the minimal election timeout. Half of the minimal election timeout is a good start.

In candidate mode, this timer is used to resend a RequestVote request message (see next chapter) if a node did not respond to the previous one.

In leader mode, this timer is used to periodically send out heartbeat messages to all followers.

\subsection{Network communication}

The communication between cluster nodes uses a remote procedure call (RPC) systems with two request and two corresponding response messages/packets. A RPC system with request/responses is required because handling of some responses require knowledge of the data sent in the request.

Clients interacting with the cluster add additional RPC calls and some optional extensions (like cluster membership) can also add additional calls.

The first message pair is RequestVote which is used in the leader election process. The request is sent out only by nodes in candidate state.
The second pair is AppendEntries which is sent out by the leader as a heartbeat message and also to append data to the distributed log[] data structure.
This message pair is used to get a consensus in the cluster. The consensus is the state of the log[] data structure saved on every node.

\subsection{RequestVote RPC call (Leader election)}
The RequestVote RPC call is used in the election process. It is sent out by the candidate node to all other nodes (broadcast). When the election timer of a node expires, the node will do the following things:

\begin{itemize}
    \item Switch to candidate state
    \item Increment its currentTerm property by one
    \item Sets votedFor property to its own node id
    \item Reset the election timer
    \item Send out RequestVote requests to all other nodes
\end{itemize}

\subsubsection*{Request}

\begin{tabularx}{\textwidth}{ | p{80px} | X | }
\hline
\textbf{Field} & \textbf{Description} \\ \hline
term & Current term of the candidate \\ \hline
candidateId & Node id of the candidate \\ \hline
lastLogIndex & Index of the last applied log entry (could be uncommitted) \\ \hline
lastLogTerm & Term of the last applied log entry (could be uncommitted) \\ \hline
\end{tabularx}

\subsubsection*{Response}

\begin{tabularx}{\textwidth}{ | p{80px} | X | }
\hline
\textbf{Field} & \textbf{Description} \\ \hline
term & The current term the answering node is in \\ \hline
voteGranted & If the answering node votes for the requesting node \\ \hline
\end{tabularx}

\subsubsection*{Request handling}

The first thing a node receiving a RequestVote request does is comparing its own term with the term in the request. If the term in the request is smaller than its own, it will reject to vote for this node by sending back a response with its current term and voteGranted flag set to false. The lower term value in the request is an indicator that the node sending the request is not up to date with the cluster and should therefore not be elected as a leader.

If the receiving node is a candidate or leader and the term in the request is higher than its own term, it will immediately switch to follower state and stop sending out heartbeat requests.
As above, a higher term value is an indicator for the \textit{receiving} node that it is behind the other nodes in the cluster and should not be a leader in this cluster.

If the currentTerm is the same as the term in the request and the votedFor property is already set to some node id, the request will also be rejected by responding with voteGranted set to false.

The last check is if the lastLogIndex and lastLogTerm are the same or higher than the latest entry in the nodes log[]. If yes, the node response with voteGranted set to true. This check is used to make sure a node is only elected as leader when it has the most recent data in the log.

If the node responds with a successful vote (voteGranted is true), it will also reset its own election timer.

\subsubsection*{Candidate response handling}

If the term received in the response is higher than its own term, the node will switch to follower mode and therefore gives up to become a leader in the current term.

If the node does not receive enough votes (because more than half of the nodes are not accessible or the other nodes have a higher term), the election timer will run out again and a new election cycle is started.

The candidate tracks its received vote answers in the receivedVotes[] associative array/map. This array
saves if a node in the cluster voted for or against election the candidate as a new leader. If
the candidate receives enough notes, it switches to leader state and starts to send out heartbeat messages. This array is also used to resend RequestVote requests to nodes which did not respond in time,
which is measured using the candidate timer described above.

\subsection{AppendEntries RPC call (Consensus)}
The AppendEntries request and response messages are used to get a consensus inside the cluster.
The AppendEntries RPC request is used by the leader as a heartbeat message and also for
inserting data into the cluster log. It receives new data from a client and sends it out
to all followers. If enough followers acknowledge that they received the data, the leader
considers the data as committed and will signal to all followers in the next AppendEntries request 
that they should also consider the data committed.

\subsubsection*{Request}

\begin{tabularx}{\textwidth}{ | p{80px} | X | }
\hline
\textbf{Field} & \textbf{Description} \\ \hline
term & The leaders currentTerm \\ \hline
leaderId & The node id of the leader \\ \hline
prevLogIndex & Index of the log entry directly before the new ones in entries[] \\ \hline
prevLogTerm & Election term of the log entry directly before the new ones in entries[] \\ \hline
entries[] & The new data to be appended to the log, blank in heartbeat messages \\ \hline
leaderCommit & The commitIndex of the leader \\ \hline
\end{tabularx}

\subsubsection*{Response}

\begin{tabularx}{\textwidth}{ | p{80px} | X | }
\hline
\textbf{Field} & \textbf{Description} \\ \hline
term & The responding nodes currentTerm \\ \hline
success & If the request succeeded \\ \hline
\end{tabularx}

\subsubsection*{Request sending}

The leader sends out AppendEntries when the heartbeat timer runs out or when a client sends new data. In both cases the request to the followers is constructed in the same way. When the request is sent out because the leader received new data from a client, it first appends the new data to its own log[]. Next, it starts to build the request to all followers: The prevLogIndex field is set to the value in nextIndex[] array for the specific node minus one, prevLogTerm is set to the corresponding term of the log entry. The entries[] field is now filled up with data from the leaders[] log starting with the index in nextIndex[] for the node.

\subsubsection*{Request handling and response sending}

As in the VoteRequest request handling, the first action is to check if the term of the
leader in the request is smaller than currentTerm of the current node. If yes, a response with success
set to false is sent back.

Next, the follower checks if it has an entry in his log with index prevLogIndex and this entry having the term prevLogTerm.
If there is an entry but the term is incorrect, the follower removes this entry and all following entries and responses
with success set to false. If the follower has no entry with this index/term, it will also answer with success set to false.
The leader will start to send the missing log entries (see "Log data catch up" section below).

If the follower has the entries referenced in prevLogIndex and prevLogTerm, it will append the data in the entries[] field to his
own log. These entries are not committed yet, because it is possible that only a minority of the nodes in the cluster has the new data.

If the leader receives enough successful AppendEntries responses, it will increase its own commitIndex and send that out in the next AppendEntries request (heartbeat or with new data) in the leaderCommit field. When the checks above (term and prevLogIndex/prevLogTerm) are successful, the follower will set its own commitIndex to the one in leaderCommit.

\subsubsection*{Response handling}

If the term in the response is higher than the currentTerm of the leader, the leader steps down immediately. The higher term
is an indicator that the leader is not up to date with the rest of the cluster.

If the success field is set to true, the leader records the highest written index (prevLogIndex + count of sent entries in the request) in its matchIndex[] array and
the same value plus one in the nextIndex[] array.

If enough nodes (the majority of the cluster including the leader) acknowledge the new data (which can be checked using the matchIndex[] array), the leader will increase its commitIndex which will then be sent out to all nodes on the next AppendEntries request.

\subsubsection*{Log data catch up}

If success in the response of the AppendEntries request is false, the data of the follower is not up to date or contains invalid data. The leader will begin sending out AppendEntries requests with lowered prevLogIndex/prevLogTerm every time until the follower finds a log[] entry matching with them. After that, the leader will start sending out log entries until the follower has the exact same log as the leader. This process uses the nextIndex[] array on the leader.





\section{Cluster membership}
Until now, the described algorithm uses a predefined cluster configuration with a given number of nodes. In a real-world system, it is desirable to
be able to add and remove nodes from the cluster. A use-case for this is e.g. replacing nodes for software updates or hardware upgrades or to
scale a cluster over more nodes for better performance.

The PhD dissertation \cite{raft_phd_thesis} describes in detail how such cluster membership changes can be implemented.
The algorithm is quite complex because it has to avoid certain failure cases when adding/removing cluster members:

\begin{itemize}
    \item Only adding one server at a time should be allowed to avoid two groups of nodes to be formed which would allow two leaders in the systems to be elected.
    \item Removing a leader should be possible without interrupting the cluster operation.
    \item Added servers should not be allowed to be elected as leaders because they have not yet caught up on the data.
\end{itemize}

The algorithm for cluster membership is not as fleshed out as the core algorithm and contains many recommendations and ideas how things could be done, and not 
concrete explanations how something should be implemented. This requires some design decisions and experimentation from the programmer when creating a real-world implementation.


\section{Client interaction}

Until now, we only describe how nodes inside the cluster communicate with each other. To make the cluster actually usable from outside the cluster, some challenges have to be solved which are described in this chapter.

\subsection{Cluster discovery}
The first challenge for the client is to find the cluster. When the cluster is static and does not support member changes (see next chapter), this can be done by simply hard-coding the IP addresses of the cluster members in the client configuration. Resolving IP addresses using the domain name system would also be a possibility. This approach can also be used with changing cluster memberships, by updating the DNS records on changes.

\subsection{Leader discovery}
Only the leader in a Raft cluster can actually add new data to the log[]. One solution for this problem is that every node which gets contacted by a client but is not a leader acts as a proxy for the client and routes the requests to the leader. Another solution is responding with the node id of the current leader. The node id of the leader is sent to all followers in the AppendEntries heartbeat message and can be sent out to clients.

\subsection{Writing data}
After finding the leader, a client sends data to leader. The leader adds the data to the Raft log and replicates the data to the cluster by sending out AppendEntries messages. Only when the leader receives an successful answer from a majority of the nodes in the cluster, it returns a success response message to the client confirming that the data has been committed.

\subsection{Duplicate writes}
Another guarantee the Raft algorithm tries to make is linearizability, which means a write of new data is executed exactly once. To implement this, a client registers with the cluster when first connecting. The cluster then creates a client session in memory. When the client sends a request to write some data, the request contains a unique request id. This request id is saved in the session. The session will be checked every time a new request from this client comes in to avoid duplicate writes from the same client by checking if the request id is already in the session.

A duplicate write can happen when a majority of the followers acknowledge the write, but the leader never sends back a success response message, because the leader crashed or the network between the client and the leader stopped to work.

For real-world implementations, it is important to remove sessions from clients which are not active any more, to avoid running out of memory. There are different approaches how to implement this, some are described in the PhD thesis\cite{raft_phd_thesis}, chapter 6.3.

\section{Scenarios}
This section takes a look on different scenarios and failure modes in the cluster and how Raft handles these cases.

The elements in the following diagrams are as follows:
\begin{itemize}
    \item Big grey circle: The cluster itself.
    \item Smaller colored circles with numbers in it: Cluster nodes.
    \item Number inside the circle: Current term of the node.
    \item Number outside the circle starting with S: Node id.
    \item Line around the circle: Election timer.
\end{itemize}


\newcommand{\raftscope}[2]{
\begin{tabular}{ p{125px}   m{11.5cm}  }
\raisebox{-\totalheight+50px}{
  \def\svgwidth{120}
  \fontsize{9pt}{11pt}\selectfont
  \input{#1}
} & #2 
\end{tabular}
}

\newcommand{\raftscopedata}[2]{
\begin{tabular}{ p{250px}   p{7cm}  }
\raisebox{-\totalheight+25px}{
  \def\svgwidth{250}
  \fontsize{9pt}{11pt}\selectfont
  \input{#1}
} & #2 
\end{tabular}
}

\subsection{Startup and leader election}


\raftscope{scenarios/election/40.pdf_tex}{
On cluster start up, there is no leader and all nodes are in the follower state.
}

\raftscope{scenarios/election/60.pdf_tex}{
After some time, one (or more, see split vote section below) node's (S1) election timer will run out.
The node will switch to candidate mode and send out RequestVote request messages (green points)
}

\raftscope{scenarios/election/65.pdf_tex}{
All followers vote for this candidate by sending back a successful RequestVote response.
}

\raftscope{scenarios/election/72.pdf_tex}{
The node S1 is now elected as leader and starts to send out AppendEntries heartbeat messages.
}


\subsection{Leader crash}
A leader crash is handled exactly like the initial leader election.
After the leader crash, one of the follower nodes election timer will run out, because the node did not receive a heartbeat (AppendEntries) message from a leader. The node then sends out 
RequestVote packets and gets elected as the new leader.

\subsection{Split vote}

\raftscope{scenarios/split_votes/53.pdf_tex}{
Once again, the cluster starts up without an elected leader. This time, one of nodes is offline (S3, greyed out) which will (with the right timing) provoke a split vote.
}

\raftscope{scenarios/split_votes/62.pdf_tex}{
Node S1 and S2 time out at nearly the same time and send out RequestVote request.
}

\raftscope{scenarios/split_votes/68.pdf_tex}{
The picture shows the RequestVote responses: S1 and S2 reject the vote for each other, because they already voted for themselves in this term. S5 votes for S2 and S4 votes for S1.
}

\raftscope{scenarios/split_votes/126.pdf_tex}{
The two candidate nodes (S1, S2) now only have two votes each (the filled out black dots inside the blue circles) and therefore do not have a majority. In the meantime, the election timer of S4 run out and this node also switched into candidate mode and sends out RequestVote requests.
}

\raftscope{scenarios/split_votes/135.pdf_tex}{
Because the term of S4 is higher than that of the other nodes, all other nodes will vote for S4 and elect it as leader.
}

\subsection{Data write}
This scenario adds a new element to the diagrams: An illustration of the Raft log[] on every node. Every row represents the Raft log[] of the node. The cells are entries in the Raft log indexed at the top. A solid line around a cell means that the data inside the cell is considered committed by this node, which means the commitIndex property is the same or higher than the index of this item. A dotted line around the cell means the item has not been committed yet.

\raftscopedata{scenarios/data_write/3.pdf_tex}{
The leader receives new data from a client and writes it into its own log[], without updating his commitIndex. Next, the leader sents out AppendEntries messages with the new data.
}

\raftscopedata{scenarios/data_write/11.pdf_tex}{
The follower nodes write the data into their own log[]. This data is not yet considered committed. The node sends back a success AppendEntries message to the leader.
}

\raftscopedata{scenarios/data_write/29.pdf_tex}{
The leader received enough AppendEntries responses from followers and increases his commitIndex to the newly written data (the line around the data cell is now solid). The leader sends out an AppendEntries heartbeat message with the updated commitIndex 
}

\raftscopedata{scenarios/data_write/34.pdf_tex}{
The followers receive the heartbeat message and update their own commitIndex to the index from the leader, sent in the leaderCommit field of the AppendEntries heartbeat.
}

\subsection{Network partition}
In a network partition, nodes get split into two or more groups. Only nodes in the same group can communicate with each other. The Raft algorithm guarantees that only the group with the majority of the nodes is operable, meaning it is able to add new data to the Raft log[]. In the following scenario, the 5 nodes from before get split up in two groups. Two clients, one connected to each group then attempts to insert data into the log[].

\raftscopedata{scenarios/network_partition/1.pdf_tex}{
Initially, the cluster has one elected leader (S4) and 5 working nodes.
}

\raftscopedata{scenarios/network_partition/2.pdf_tex}{
A network partition is introduced between the nodes S1, S2 and S5 and the nodes S4 and S3.
Because the nodes S1, S2 and S5 do not receive heartbeats from the now cut off leader S4, they elect S1 as their new leader
}

\raftscopedata{scenarios/network_partition/3.pdf_tex}{
A client connected to the S3/S4 partition tries to insert some data into the cluster. Is sends its data to the leader,
which tries to send the changes to all nodes.
}
\raftscopedata{scenarios/network_partition/4.pdf_tex}{
Only the S3 receives the AppendEntries request. It acknowledges the write with a successful AppendEntries response (orange circle with a + in it). Because the leader never finds a majority of node sacknowledging the change, the data write is never committed (the lines around the data entry in the log diagram stay dotted).
}

\raftscopedata{scenarios/network_partition/5.pdf_tex}{
A client connected to the other network partition inserts data into the cluster by sending it to the leader S1. The leader S1 sends out AppendEntries requests to all nodes.
}

\raftscopedata{scenarios/network_partition/6.pdf_tex}{
Nodes S2 and S5 acknowledge the write by sending back a successful AppendEntries response.
}

\raftscopedata{scenarios/network_partition/7.pdf_tex}{
The master receives the two acknowledgements of the writes and has therefore a majority for the change. It marks the change as committed by increasing its own commitIndex property (the line around the log entry is now solid). The master also sends out his new commitIndex as leaderIndex in the next heartbeat.
}

\raftscopedata{scenarios/network_partition/8.pdf_tex}{
The nodes S2 and S5 receive the new commitIndex from the master and set their local commitIndex to this new value.
}

\raftscopedata{scenarios/network_partition/9.pdf_tex}{
The network partition is healed and all nodes can now talk to each other again. Both leaders send out heartbeat (AppendEntries request) messages to all nodes.
The message also contains the data written by the seconds client (the pink cell with the 3 in the diagram) for the nodes who do not have the data yet (S3, S4).
}

\raftscopedata{scenarios/network_partition/10.pdf_tex}{
The leader S2 receives the heartbeat message from S1 and steps down, because the term in the message is higher than his own. 
Because the heartbeat message already contains data, the nodes S3 and S4 can discard the old uncommitted data with the same index and override
it with the new data from the message.
}

\section{Implementation}

The author of this paper implemented the core Raft algorithm (communication inside the cluster) in Python. The main challenges were not implementing the algorithm logic itself, but all the challenges around network and system programming:
\begin{itemize}
    \item Correctly handling timeouts and retrying requests
    \item Multithreaded programming with sockets and timers
    \item Locking of the state between threads
\end{itemize}

At least for the core algorithm, Raft is very well described and was not the main challenge to implement the software.
Other parts like the client interaction and cluster membership are described in lesser detail and would require more design decissions by the programmer
in comparison to the core algorithm, which can be written up more or less directly from the paper

Another challenge that applied to all network based system programming is testing. It is extremely difficult to reliably test such systems, because simulating errors like
delayed or lost packets, node downtime or network partitions is hard. There are approaches to simulate the complete system to validate a distributed systems,
which are very complex to build and slow to execute, but can guarantee a very high level of correctness\cite{foundationdb_testing}.

An example for a problem showing up in a real-world implementations which is very hard to find and write tests for is inconsistent/stale reads. As shown in the "Network partition" scenario above, it is possible that a Raft cluster has two active leaders. If in this scenario a client on the majority part of the network updates a value in the Raft log, it is possible that a client reading from the other partition reads the old value. A fix for this problem is that the leader has to wait for a majority of the nodes to acknowledge the leaders heartbeat request, before the leader answers the client. This signals the leader that he is still a legitimate leader and that he is allowed to authoritatively answer the client's request.  This of course a big performance degradation for something that happens so seldom. The problem showed up both in etcd and Consul \cite{aphyr_call_me_maybe}, and was solved by providing a flag on the request which can be set if this problem has to be avoided at all cost \cite{etcd_consistency} \cite{consul_consistency}.


\section{Conclusion}
Today, there are multiple real-world implementations of distributed systems using the Raft algorithm, proving that one of the main goals of Raft have been accomplished.

Kubernetes, a container management system initially developed by Google and now having a broad support in the IT community uses etcd, a key-value store written in Go and implementing the Raft algorithm, as its only database. Consul, a similar service to etcd can be used as a replacement in Kubernetes. Consul also uses the Raft algorithm.
Both systems are CP systems as described in the Raft paper.

Many newer databases from the NoSQL movement are also using the Raft algorithm, including MongoDB \cite{mongodb_raft}, CockroachDB\cite{cockroachdb_raft} and RethinkDB\cite{rethinkdb_raft}. For performance reasons, some of these databases use Raft for coordination inside the cluster, but not for the actual data replication\cite[Development section]{rethinkdb_raft_metadata}.

Diego Ongaro did a user study for his PhD thesis to prove that the algorithm is easier to understand than Paxos \cite[Raft user study chapter]{raft_phd_thesis}. The author of this paper also thinks that the algorithm is understandable, but by no means simple. Besides the original paper and the PhD thesis, the visualization Raftscope \cite{raftscope} and The Secret Lives of Data \cite{the_secret_lives_of_data} helped quite a bit, as did the source code of LogCabin\cite{logcabin}, the Raft implementation Ongaro wrote for his thesis.

\bibliographystyle{plain}
\bibliography{bibliography} 

\end{document}
