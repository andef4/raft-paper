\input{style.tex}

\begin{document}

%%
%% begin titlepage
%%

\begin{titlepage}
\newgeometry{left=6cm}
\definecolor{titlepage-color}{HTML}{2196F3}
\newpagecolor{titlepage-color}\afterpage{\restorepagecolor}
\newcommand{\colorRule}[3][black]{\textcolor[HTML]{#1}{\rule{#2}{#3}}}
\begin{flushleft}
\noindent
\\[-1em]
\color[HTML]{FFFFFF}
\makebox[0pt][l]{\colorRule[1976D2]{1.3\textwidth}{8pt}}
\par
\noindent

{ \setstretch{1.4}
\vfill
\noindent {\huge \textbf{\textsf{The Raft Consensus Algorithm}}}
\vskip 2em
\noindent
{\Large \textsf{\MakeUppercase{Fabio Anderegg}}
\vfill
}

\textsf{2017-12-04}}
\end{flushleft}
\end{titlepage}
\restoregeometry

%%
%% end titlepage
%%


{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
\pagebreak
}

\section{Abstract}

Raft is a consensus algorithm for distributed systems comparable, but much simpler to understand and implement than Paxos.
This paper explains the algorithm from the bottom up, first describing all the components and
then explaining how they work together to solve specific scenarios.
There are some proposed extensions to Raft which are useful when implementing a real world distributed system which are examined in the paper.

\section{Introduction}

\includegraphics[scale=0.05]{raft}

This paper covers Raft, a consensus algorithm for distributed systems.

Raft wants to be a simpler replacement for PAXOS, an algorithm developed by Leslie Lamport and considered the reference consensus algorithm for distributed systems\cite{paxos}. PAXOS suffers from two big problems, which Raft tries to solve: It is hard to understand \cite[p.91]{raft_phd_thesis}, has some missing pieces for a real world system and is generally hard to implement\cite{paxos_made_live}.

Raft was developed by Diego Ongaro and extensively described in this PhD thesis \cite{raft_phd_thesis}. He not only describes how the algorithm works, but also did a study under students to test if the algorithm is actually easier to understand than PAXOS. Raft has been formally verified \cite{raft_proof} and implemented in multiple real world systems, e.g. in etcd \cite{etcd_raft}, Consul \cite{consul_raft} and RethinkDB \cite{rethinkdb_raft}.

The offical GitHub Page \cite{raft_github} contains many further references about the algorithm, a simulator for the algorithm from which screenshots have been used in this paper and an extensive list of implementations in many different programming languages.

This paper has three main parts: The first part explains what consensus in a distributed system means, the second part explains how the Raft algorithm works in detail and the third part shows how the algorithm reacts to some specific scenarios.
The pictures for the scenarios sections were created using Raftscope \cite{raftscope}, a software to visualize the state of Raft-based distributed system.

This paper describes the latest version of Raft as described in Diego Ongaros PhD dissertation \cite{raft_phd_thesis}. Compared to the papers published before the dissertation, the dissertation describes the client-cluster interaction much more clearly and contains a simpler cluster membership change algorithm.

\subsection{What is consensus?}

Consensus in a distributed system means that multiple servers agree on the same specific state of the system. After agreeing on a state, that state cannot be changed any more, but the system can agree on a new state superseding the old state.

Clients interacting with such a system see a single consistent state, independent of the server they are talking to.

This problem is hard to solve because most components participating in the system (servers, network, clocks, disk...) are inherently unreliable. They can not only fail and be complete unusable, they can also be slow, mangle data in transit (network packets ordering) or contain invalid data (bit flipping in RAM).

Many consensus algorithms (including Raft) are based on a majority system. If a majority of the servers agree on a specific value, that value is considered the "true" value of the system.

\subsection{The CAP Theorem}
The CAP theorem describe how certain failures in a distributed system affect the system. It states that a distributed system can never provide more than two of the following guarantees: \cite{cap_theorem}

\textbf{Consistency}: Operations in the system must appear as if they happened one after another in total order. At a specific time all servers must return the same, most recent write to the system.

\textbf{Availability}: Every working node in the system must be able to return a response to a request.

\textbf{Partition tolerance}: The system works even when the network drops packets, loses nodes or is partitioned.

Raft is a CP algorithm, that means systems using this algorithm are tolerant to partitions and always return a consistent view of the data in the system. Raft sacrifices the availability guarantee, which means that at least a majority of the servers must be working and be able to communicate with each other for the system to be usable.

\section{The algorithm}
A Raft cluster consists of nodes. A node is a server running the Raft software and communicating with the other nodes in the cluster over a network connection.
In every cluster, one node is elected as a leader node, which coordinates the operations inside the cluster. A leader can only be elected by
a majority of the cluster nodes. This means the minimal cluster size is three nodes, and bigger clusters should always have an uneven count of nodes (In a cluster
of six nodes, two nodes can crash and a majority can still be found, this is not better than a five node cluster).
This also means that when more than half of the cluster nodes are offline, the cluster is inoperable. This is availability guarantee of the CAP theorem the Raft algorithm gives up
for the other two guarantees. 

The current leader sends out a heartbeat network packet in some preconfigured interval. If a follower node does not receive a heartbeat within a given interval (which should be a bit longer than the interval in which the master sends out the heartbeats to account for the network round-trip time), the node considers the master to be down an proposes ifself as a new leader.

The main data structure of the algorithm as a replicated, append-only log. It contains the data the clients wants to save in the system and also some metadata
for the algorithm itself.

The Raft algorithm is designed to survive the following and many other failure cases:
\begin{itemize}
    \item The current leader crashes
    \item Network connections between nodes fail
    \item Network connections are very slow or drop messages between nodes
\end{itemize}

\subsection{State}
Every node in the Raft cluster is always in one of the following three states:
\begin{itemize}
    \item Leader: The node is the leader of the cluster
    \item Candidate: The node did not receive a heartbeat from a leader, it is waiting to get enough votes to be a leader itself
    \item Follower: Some other node is a leader
\end{itemize}

\subsubsection*{Persistent data written to disk}

This data has to be saved on disk before replying to the RPC call from the leader. The fsync or equivalent system call should be used on the file descriptor to make sure the data has arrived on the disk and not just the write caches of the operating system [CN].

\begin{tabular}{ | l | p{13.7cm} | }
\hline
currentTerm & The election term the node thinks is the current one \\ \hline
votedFor & For which node the current node has voted in this term \\ \hline
log[] & The replicated log containing all the data clients want to save in the system. The log is append only, only new entries can be append but old ones can not be changed.
Every log entry contains an index (starting at one) and the term in which the data was written. \\ \hline
\end{tabular}

\subsubsection*{Volatile data}

This data is not written to disk and initialized on node startup.

\begin{tabular}{ | l | p{13.7cm} | }
\hline
commitIndex & Up to which index in the replicated log the current node considers to be committed (commited means a majority of the cluster
received the data entry, saved it to disk and notified the leader). On startup, this information is initialized from the leaders heartbeat message (AppendEntry). \\ \hline
lastApplied & The highest index in the replicated log, initialized when reading the log entries from disk on startup. \\ \hline
\end{tabular}

\subsubsection*{Leader data}

This data is initialized when the node gets elected as leader and is not persisted to disk.
It has to be cleared every time the node gets elected and must not be reused in later terms.

\begin{tabular}{ | l | p{13.7cm} | }
\hline
nextIndex[] & TODO copy from paper: When a leader first comes to power, it initializes all nextIndex values to the index just after the last one in its log (11 in Figure 7).  \\ \hline
matchIndex[] & TODO \\ \hline
\end{tabular}

\subsection{Timers}
The Raft server needs two timers which when they run out trigger some action.

\subsubsection*{Election timer}


\subsubsection*{Candiate and leader timer}
* faster than election timer
* timer on the leader to send out heartbeat messages
* timer to resend messages for voting

\subsection{Network communication}

two way rpc
important to always send messages/reconnect/retry, even when node was down

\textbf{RequestVote}

\textbf{RequestVote Response}

\textbf{AppendEntries}

\textbf{AppendEntries Response}

\section{Scenarios}
This section takes a look on different scenarios and failure modes in the cluster and how Raft handles these cases.

\subsection{Startup and leader election}

\subsection{Leader crash}
A leader crash is handled exactly like the initial leader election.
After the leader crash, one of the follower nodes election timer will run out, because the node did not receive a heartbeat (AppendEntry) message from a leader. The node then sends out 
RequestVote packets and gets elected as the new leader.

\subsection{Data write}
Writing data into the cluster uses the same message as the heartbeat.
* send out data
* reply with success
* update commit index in next appendentry heartbeat
* nodes update commit index

\subsection{Follower with old data reconnects to the cluster}
* receives heartbeat message
* prevLogIndex is more recent that lastApplied => reply with success=false to heartbeat message
* leader sends appendEntry with lower index until it finds an index replicated by this node
* leader will then send out appendEntries requests until the node has again all the data (probably in one packet if AppendEntry can contain multiple entries)

\subsection{Old leader returns}
=> Steps down as soon as they see a heartbeat packet from the current master with a newer term.
The node is a follower now and does the same as in the section above to regain all the missing data.
* deleted old uncommited data

\subsection{Switching masters in network partition}
When connection between two nodes fail, a ping-pong between these two nodes *can* happen.

http://openlife.cc/system/files/4-modifications-for-Raft-consensus.pdf

\section{Cluster membership}

\section{Client interaction}

* duplicate writes



\subsection{Inconsistens/stale reads}
(adressed in paper)
https://github.com/coreos/etcd/issues/741

https://aphyr.com/posts/316-call-me-maybe-etcd-and-consul
https://gist.github.com/armon/11059431

\section{Conclusion}
References the abstract

\bibliographystyle{plain}
\bibliography{bibliography} 

\end{document}
